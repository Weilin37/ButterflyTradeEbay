{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = 'Endangered Current Sept'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_search(search_term):\n",
    "    # Initial query\n",
    "    search_term = '+'.join(search_term.split(' '))\n",
    "    URL = 'https://www.ebay.com/sch/i.html?_sacat=0&_udlo&_udhi&_ftrt=901&_ftrv=1&_sabdlo&_sabdhi&_samilow&_samihi&_sop=12&_dmd=1&_ipg=200&_fosrp=1&_nkw=%22'+search_term+'%22&rt=nc&LH_PrefLoc=2&_trksid=p2045573.m1684'\n",
    "    page = requests.get(URL)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    # Get number of results\n",
    "    try:\n",
    "        count = soup.find('div', id='cbelm').find_all('span', class_='rcnt')[0].text\n",
    "    except:\n",
    "        count = soup.find_all('span', class_='nllclt')[0].find_all('b')[0].text\n",
    "            \n",
    "    #If no results are returned then skip it\n",
    "    if count == '0':\n",
    "        return URL, count, False\n",
    "\n",
    "    #If no results are returned then skip it\n",
    "    results = soup.find('ul', id='ListViewInner')\n",
    "\n",
    "    # Get listing items\n",
    "    items = results.find_all('li', class_='sresult', recursive=False)\n",
    "\n",
    "    # Capture all data in this list\n",
    "    dict_list = []\n",
    "\n",
    "    # Loop through listings\n",
    "    for item in items:\n",
    "        # Define empty dictionary\n",
    "        dict_item = {\n",
    "            'id': None,\n",
    "            'title': None,\n",
    "            'link': None,\n",
    "            'price': None,\n",
    "            'origin': None,\n",
    "            'image': None,\n",
    "            'seller_name': None,\n",
    "            'seller_link': None,\n",
    "            'sell_type': None,\n",
    "            'num_bids': 0,\n",
    "            'num_watchers': 0,\n",
    "            'num_stock': 1,\n",
    "            'date': None,\n",
    "            #'description': None\n",
    "        }\n",
    "        \n",
    "        # Grab all the high level data\n",
    "        id = item.get('id')\n",
    "        title_and_link = item.find_all('a', class_='vip')[0]\n",
    "        title = title_and_link.get('title').replace('Click this link to access ','')\n",
    "        link = title_and_link.get('href')\n",
    "        price = item.find_all('li', class_='lvprice')[0].find_all('span')[0].text.strip()\n",
    "        try:\n",
    "            image = item.find_all('img', class_='img')[0].get('src')\n",
    "        except:\n",
    "            image = 'NA'\n",
    "\n",
    "        # Detailed Scraping\n",
    "        try:\n",
    "            deeper_info = BeautifulSoup(requests.get(link).content, 'html.parser')\n",
    "        except:\n",
    "            print(link)\n",
    "        \n",
    "        # Get seller info\n",
    "        try:\n",
    "            seller_info = deeper_info.find_all('div', class_='si-inner')[0]\n",
    "            seller_name = seller_info.find_all('span', class_='mbg-nw')[0].text\n",
    "            seller_link = seller_info.find_all('a')[0].get('href')\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        # Get locality\n",
    "        try:\n",
    "            origin = deeper_info.find_all('div', id='itemLocation')[0].find_all('span')[0].text.strip()\n",
    "        except:\n",
    "            origin = 'Unknown'\n",
    "        \n",
    "        # Get sales type, num bids, num stock\n",
    "        try:\n",
    "            num_bids = deeper_info.find('a', id='vi-VR-bid-lnk').find('span', id='qty-test').text.strip()\n",
    "            sell_type = 'Auction'\n",
    "            num_stock = 1\n",
    "        except:\n",
    "            sell_type = 'Instant'\n",
    "            num_bids = 0\n",
    "            try:\n",
    "                num_stock = deeper_info.find('span', id='qtySubTxt').find_all('span')[0].text.strip().replace(' available','')\n",
    "            except:\n",
    "                num_stock = 1\n",
    "        \n",
    "        # Get num watchers\n",
    "        try:\n",
    "            num_watchers = deeper_info.find('div', id='why2buy').find_all('span', class_='w2b-sgl')[0].text.strip().replace(' watchers')\n",
    "        except:\n",
    "            num_watchers = 0\n",
    "            \n",
    "        # Get starting date\n",
    "        try:\n",
    "            bid_link = deeper_info.find('a', id='vi-VR-bid-lnk').get('href')\n",
    "            bid_page = requests.get(bid_link)\n",
    "            bid_info = BeautifulSoup(bid_page.content, 'html.parser')\n",
    "            date = bid_info.find_all('table', class_='ui-component-table_wrapper')[0].find_all('tr')[-1].find_all('td')[-1].get_text().strip()\n",
    "        except:\n",
    "            try:\n",
    "                results = deeper_info.find('div', id='vi-desc-maincntr').find_all('div', class_='vi-desc-revHistory')[0].parent()[5].get_text().strip()\n",
    "                date = results.replace('Last updated on','').replace('View all revisions','').strip()\n",
    "            except:\n",
    "                date = 'Unknown'\n",
    "\n",
    "        # Description\n",
    "        #description_src = deeper_info.find_all('iframe')[0].get('src')\n",
    "        #description = ' '.join(BeautifulSoup(requests.get(description_src).content, 'html.parser').find('div', id='ds_div').text.strip().split())\n",
    "\n",
    "        # Put the data into the dictionary and append to the list\n",
    "        dict_item['id'] = id\n",
    "        dict_item['title'] = title\n",
    "        dict_item['link'] = link\n",
    "        dict_item['price'] = price\n",
    "        dict_item['image'] = image\n",
    "        dict_item['origin'] = origin\n",
    "        dict_item['seller_name'] = seller_name\n",
    "        dict_item['seller_link'] = seller_link\n",
    "        dict_item['sell_type'] = sell_type\n",
    "        dict_item['num_watchers'] = num_watchers\n",
    "        dict_item['num_bids'] = num_bids\n",
    "        dict_item['num_stock'] = num_stock\n",
    "        dict_item['date'] = date\n",
    "        #dict_item['description'] = description\n",
    "\n",
    "        dict_list.append(dict_item)\n",
    "\n",
    "    # convert data to pandas dataframe and save species search results\n",
    "    df = pd.DataFrame(dict_list)\n",
    "    df.to_csv(base_folder+\"/current sales/current_sales_\"+search+\".csv\", index=False, encoding='utf-8-sig')\n",
    "\n",
    "    return URL, count, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 9282/9425 [15:45<03:28,  1.46s/it]   "
     ]
    }
   ],
   "source": [
    "# Get today's date\n",
    "date = '2021-08-30'\n",
    "\n",
    "# Load file containing past scraps so that we can resume where we left off if we stopped\n",
    "try:\n",
    "    done_list = pd.read_csv(base_folder+'/current_search_result_'+date+'.csv')\n",
    "    done = done_list['species'].tolist()\n",
    "    done_list = done_list.to_dict('records')\n",
    "except:\n",
    "    done_list = []\n",
    "    done = []\n",
    "\n",
    "# Get list of species\n",
    "search_list = pd.read_csv(base_folder+'/endangered_species_list.csv')['Species'].tolist()\n",
    "for search in tqdm(search_list):\n",
    "    if search in done:\n",
    "        continue\n",
    "        \n",
    "    # Get scrape results\n",
    "    URL, count, result = get_search(search)\n",
    "    \n",
    "    # Append scrap results to the list\n",
    "    search_result_dict = {'species': None, 'found': None, 'URL': None, 'count': None}\n",
    "    search_result_dict['URL'] = URL\n",
    "    search_result_dict['species'] = search\n",
    "    search_result_dict['found'] = result\n",
    "    search_result_dict['count'] = count\n",
    "    done_list.append(search_result_dict)\n",
    "    \n",
    "    # Save metadata\n",
    "    df_result = pd.DataFrame(done_list)\n",
    "    df_result.to_csv(base_folder+\"/current_search_result_\"+date+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
