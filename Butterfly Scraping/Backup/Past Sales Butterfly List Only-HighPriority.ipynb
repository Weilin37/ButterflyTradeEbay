{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import sys\n",
    "from bs4 import BeautifulSoup\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import re\n",
    "from proxycrawl import CrawlingAPI, ScraperAPI, LeadsAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = 'Butterfly Past High Priority June'\n",
    "api = CrawlingAPI({ 'token': 'ubxeiSnB52uiJVjGDWSa0g&url=https%3A%2F%2Fwww.amazon.com%2Fdp%2FB00JITDVD2' })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_search(search):\n",
    "    # Initial query\n",
    "    \n",
    "    search_term = '+'.join(search.split(' '))\n",
    "    URL = 'https://www.ebay.com/sch/i.html?_sacat=0&LH_Sold=1&_udlo=&_udhi=&_samilow=&_samihi=&_sop=12&_dmd=1&_ipg=200&LH_Complete=1&_fosrp=1&_nkw=%22'+search_term+'%22&rt=nc&LH_PrefLoc=2'\n",
    "    page = api.get(URL,{'country':'US'})\n",
    "    soup = BeautifulSoup(page['body'], 'html.parser')\n",
    "    \n",
    "    found = False\n",
    "    cnt = 0\n",
    "    while not found:\n",
    "        if cnt > 10:\n",
    "            return URL, 0, False, cnt\n",
    "        cnt = cnt+1\n",
    "        \n",
    "        page = api.get(URL,{'country':'US'})\n",
    "        soup = BeautifulSoup(page['body'], 'html.parser')\n",
    "        try:\n",
    "            soup.find('div', id='cbelm').find_all('span', class_='rcnt')[0].text\n",
    "            found = True\n",
    "        except:\n",
    "            found = False\n",
    "        \n",
    "    \n",
    "    # Get number of results\n",
    "    try:\n",
    "        count = soup.find('div', id='cbelm').find_all('span', class_='rcnt')[0].text\n",
    "    except:\n",
    "        try:\n",
    "            count = soup.find_all('span', class_='nllclt')[0].find_all('b')[0].text\n",
    "        except:\n",
    "            print(URL)\n",
    "            \n",
    "            count = soup.find_all('span', class_='rcnt')[0].text\n",
    "        \n",
    "    #If no results are returned then skip it\n",
    "    if count == '0':\n",
    "        return URL, count, False, cnt\n",
    "\n",
    "    # Get the main list container of the results\n",
    "    results = soup.find('ul', id='ListViewInner')\n",
    "\n",
    "    # Get listing items\n",
    "    items = results.find_all('li', class_='sresult', recursive=False)\n",
    "\n",
    "    # Capture all data in this list\n",
    "    dict_list = []\n",
    "\n",
    "    # Loop through listings\n",
    "    for item in items:\n",
    "        # Define empty dictionary\n",
    "        dict_item = {\n",
    "            'id': None,\n",
    "            'title': None,\n",
    "            'link': None,\n",
    "            'image': None,\n",
    "            'price_sold': None,\n",
    "            'end_date': None,\n",
    "            'origin': None,\n",
    "            'seller_name': None,\n",
    "            'seller_link': None,\n",
    "            'sell_type': None,\n",
    "            'num_bids': 0,\n",
    "            'num_sold': 1,\n",
    "            #'description': None\n",
    "        }\n",
    "\n",
    "        # Grab all the high level data data\n",
    "        id = item.get('id')\n",
    "        title_and_link = item.find_all('a', class_='vip')[0]\n",
    "        title = title_and_link.get('title').replace('Click this link to access ','')\n",
    "        link = title_and_link.get('href')\n",
    "        price_info = item.find_all('ul', class_='lvprices', recursive=False)[0].find_all('li', class_='lvprice')[0]\n",
    "        price_best_offer = price_info.find_all('span', class_='sboffer')\n",
    "        price_bid_sold = price_info.find_all('span', class_='bidsold')\n",
    "        end_date = item.find_all('ul', class_='lvdetails', recursive=False)[0].find_all('span', class_='tme')[0].find_all('span')[0].text.strip()\n",
    "        \n",
    "        try:\n",
    "            image = item.find_all('img', class_='img')[0].get('src')\n",
    "        except:\n",
    "            image = 'NA'\n",
    "\n",
    "\n",
    "        # Description\n",
    "        #description_src = deeper_info.find_all('iframe')[0].get('src')\n",
    "        #description = ' '.join(BeautifulSoup(requests.get(description_src).content, 'html.parser').find('div', id='ds_div').text.strip().split())\n",
    "\n",
    "        # Put the data into the dictionary and append to the list\n",
    "        dict_item['id'] = id\n",
    "        dict_item['title'] = title\n",
    "        dict_item['link'] = link\n",
    "        dict_item['image'] = image\n",
    "        if len(price_best_offer) > 0:\n",
    "            dict_item['price_sold'] = price_best_offer[0].text.strip()\n",
    "        else:\n",
    "            dict_item['price_sold'] = price_bid_sold[0].text.strip()\n",
    "        dict_item['end_date'] = end_date\n",
    "\n",
    "        dict_list.append(dict_item)\n",
    "\n",
    "    # convert data to pandas dataframe and save species search result to CSV\n",
    "    df = pd.DataFrame(dict_list)\n",
    "    df.to_csv(base_folder+\"/past sales/past_sales_\"+search+\".csv\", index=False, encoding = 'utf-8-sig')\n",
    "    \n",
    "    return URL, count, True, cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 21/500 [02:32<46:50,  5.87s/it]  "
     ]
    }
   ],
   "source": [
    "# Get today's date\n",
    "date = '2022-06-01'\n",
    "part = 1\n",
    "\n",
    "# Load file containing past scraps so that we can resume where we left off if we stopped\n",
    "try:\n",
    "    done_list = pd.read_csv(base_folder+'/past_search_result_'+str(part)+'_'+date+'.csv')\n",
    "    done = done_list['species'].tolist()\n",
    "    done_list = done_list.to_dict('records')\n",
    "except:\n",
    "    done_list = []\n",
    "    done = []\n",
    "\n",
    "# Get list of species\n",
    "search_list = pd.read_csv(base_folder+'/species.csv')['Species'].tolist()\n",
    "search_list = search_list[(500*(part-1)):(500*part)]\n",
    "\n",
    "# Loop through list of species, making sure to skip the ones we already finished\n",
    "for search in tqdm(search_list):\n",
    "    if search in done:\n",
    "        continue\n",
    "    # Get scrape results\n",
    "    URL, count, result, captcha_cnt = get_search(search)\n",
    "    \n",
    "    # Append scrape results to the list\n",
    "    search_result_dict = {'species': None, 'found': None, 'URL': None, 'count': None, 'captcha_count':None}\n",
    "    search_result_dict['URL'] = URL\n",
    "    search_result_dict['species'] = search\n",
    "    search_result_dict['found'] = result\n",
    "    search_result_dict['count'] = count\n",
    "    search_result_dict['captcha_count'] = captcha_cnt\n",
    "    done_list.append(search_result_dict)\n",
    "    \n",
    "    # Save metadata\n",
    "    df_result = pd.DataFrame(done_list)\n",
    "    df_result.to_csv(base_folder+\"/past_search_result_\"+str(part)+'_'+date+\".csv\", index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
